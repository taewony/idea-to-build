## 소개: 나만의 작은 언어모델 실행하기

로컬 언어모델(Local Language Model)의 세계에 오신 것을 환영합니다!
이 가이드는 리눅스 환경에서 직접 **Small Language Model(SLM)** 을 설치하고 실행하는 과정을 단계별로 안내합니다.
이 과정을 마치면, 여러분은 자신의 하드웨어에서 인공지능 모델을 구동할 뿐 아니라,
그 모델과 대화할 수 있는 **웹 기반 채팅 서비스**까지 직접 구현하게 됩니다.

---

### 학습 목표

* **환경 설정**: Conda를 이용해 깨끗하고 안정적인 실행 환경을 구축하고, PyTorch 등 필수 라이브러리를 설치하는 방법을 배웁니다.
* **모델 선택**: Phi-3, Llama 3, Gemma 등 대표적인 오픈소스 소형 언어모델을 비교하고, 자신의 하드웨어 환경에 맞는 모델을 선택하는 법을 익힙니다.
* **로컬 설치**: `ollama`나 Hugging Face의 `transformers` 라이브러리를 이용해 모델 가중치를 다운로드하고, 로컬 환경에서 직접 실행하는 절차를 따라갑니다.
* **채팅 서비스 구축**: Python의 Flask 또는 FastAPI를 활용해, 로컬에서 구동 중인 LLM을 API 형태로 외부에 노출하는 간단한 백엔드 서비스를 만듭니다.
* **프론트엔드 구현**: HTML과 JavaScript로 기본적인 웹 인터페이스를 작성하여, 사용자가 입력한 프롬프트를 모델로 전송하고 응답을 화면에 표시하는 방법을 배웁니다.

---

### 사전 준비 사항

* 리눅스 배포판(예: Ubuntu 22.04)이 설치된 컴퓨터
* 리눅스 명령어에 대한 기본적인 이해
* Python 3.10 이상 설치
* (선택 사항이지만 권장) CUDA가 설치된 NVIDIA GPU — 모델 실행 성능을 크게 향상시킵니다

---

이 실습 중심의 과정은 "**강력한 인공지능 모델을 직접 다루는 과정을 단순화**"하는 데 초점을 맞추고 있습니다.
학습을 마치면, 자신만의 맞춤형 AI 응용 프로그램을 구축할 수 있는 기본기를 갖추게 될 것입니다.

이제 시작해봅시다!
